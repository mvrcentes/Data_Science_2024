{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importación de librerías"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","import tensorflow as tf\n","import numpy as np\n","\n","# Cargar datos con un límite de palabras\n","(train_texts, train_labels), (test_texts, test_labels) = imdb.load_data(num_words=50000)\n","\n","# Ajuste de longitud de secuencias\n","train_texts_padded = sequence.pad_sequences(train_texts, maxlen=150)\n","test_texts_padded = sequence.pad_sequences(test_texts, maxlen=150)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Preparación del índice de palabras y listas de palabras clave"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Obtención e inversión del índice de palabras para decodificación\n","word_index = tf.keras.datasets.imdb.get_word_index()\n","inverted_index = {i + 3: word for word, i in word_index.items()}\n","inverted_index[1] = \"[START]\"\n","inverted_index[2] = \"[OOV]\"\n","\n","# Listas de palabras con connotación positiva y negativa\n","positive_set = {\"good\", \"great\", \"excellent\", \"amazing\", \"awesome\", \"fantastic\"}\n","negative_set = {\"bad\", \"terrible\", \"horrible\", \"worst\", \"awful\"}\n"]},{"cell_type":"markdown","metadata":{},"source":["# Extracción y escalado de características"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","def calculate_features(sequences):\n","    calculated_features = []\n","    for seq in sequences:\n","        decoded = [inverted_index.get(i, \"\") for i in seq]\n","        positive_hits = sum(word in positive_set for word in decoded)\n","        negative_hits = sum(word in negative_set for word in decoded)\n","        total_words = len(decoded)\n","        ratio = (positive_hits - negative_hits) / total_words if total_words > 0 else 0\n","        calculated_features.append([len(seq), ratio])\n","    return np.array(calculated_features)\n","\n","# Extracción y normalización de características\n","features_train = calculate_features(train_texts_padded)\n","features_test = calculate_features(test_texts_padded)\n","\n","scaler = StandardScaler()\n","features_train_scaled = scaler.fit_transform(features_train)\n","features_test_scaled = scaler.transform(features_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Construcción y compilación del modelo"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import Input, Concatenate, LSTM, Dense, Embedding, Dropout\n","from tensorflow.keras.models import Model\n","\n","sequence_input = Input(shape=(150,))\n","embedded_sequence = Embedding(50000, 128)(sequence_input)\n","lstm_layer = LSTM(128, return_sequences=True)(embedded_sequence)\n","lstm_layer = Dropout(0.3)(lstm_layer)\n","lstm_layer = LSTM(64, return_sequences=False)(lstm_layer)\n","lstm_layer = Dropout(0.3)(lstm_layer)\n","\n","feature_input = Input(shape=(2,))\n","merged_layers = Concatenate()([lstm_layer, feature_input])\n","\n","dense_layers = Dense(64, activation='relu')(merged_layers)\n","dense_layers = Dropout(0.5)(dense_layers)\n","dense_layers = Dense(32, activation='relu')(dense_layers)\n","output_layer = Dense(1, activation='sigmoid')(dense_layers)\n","\n","model = Model(inputs=[sequence_input, feature_input], outputs=output_layer)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Entrenamiento y evaluación del modelo"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","782/782 - 163s - 208ms/step - accuracy: 0.7940 - loss: 0.4427 - val_accuracy: 0.8641 - val_loss: 0.3273\n","Epoch 2/15\n","782/782 - 168s - 215ms/step - accuracy: 0.9174 - loss: 0.2280 - val_accuracy: 0.8600 - val_loss: 0.3323\n","Epoch 3/15\n","782/782 - 169s - 216ms/step - accuracy: 0.9544 - loss: 0.1307 - val_accuracy: 0.8450 - val_loss: 0.5018\n","Epoch 4/15\n","782/782 - 170s - 217ms/step - accuracy: 0.9746 - loss: 0.0759 - val_accuracy: 0.8441 - val_loss: 0.4903\n","Epoch 5/15\n","782/782 - 170s - 218ms/step - accuracy: 0.9854 - loss: 0.0460 - val_accuracy: 0.8423 - val_loss: 0.6183\n","Epoch 6/15\n","782/782 - 171s - 219ms/step - accuracy: 0.9892 - loss: 0.0351 - val_accuracy: 0.8366 - val_loss: 0.5982\n","Epoch 7/15\n","782/782 - 190s - 243ms/step - accuracy: 0.9894 - loss: 0.0349 - val_accuracy: 0.8382 - val_loss: 0.9565\n","Epoch 8/15\n","782/782 - 210s - 268ms/step - accuracy: 0.9929 - loss: 0.0236 - val_accuracy: 0.8322 - val_loss: 0.7305\n","Epoch 9/15\n","782/782 - 228s - 292ms/step - accuracy: 0.9950 - loss: 0.0155 - val_accuracy: 0.8300 - val_loss: 0.8209\n","Epoch 10/15\n","782/782 - 214s - 273ms/step - accuracy: 0.9967 - loss: 0.0113 - val_accuracy: 0.8230 - val_loss: 1.3594\n","Epoch 11/15\n","782/782 - 231s - 295ms/step - accuracy: 0.9956 - loss: 0.0158 - val_accuracy: 0.8359 - val_loss: 0.8490\n","Epoch 12/15\n","782/782 - 233s - 298ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 0.8356 - val_loss: 1.0735\n","Epoch 13/15\n","782/782 - 205s - 262ms/step - accuracy: 0.9981 - loss: 0.0079 - val_accuracy: 0.8394 - val_loss: 0.9797\n","Epoch 14/15\n","782/782 - 173s - 221ms/step - accuracy: 0.9967 - loss: 0.0104 - val_accuracy: 0.8340 - val_loss: 1.1310\n","Epoch 15/15\n","782/782 - 180s - 231ms/step - accuracy: 0.9971 - loss: 0.0099 - val_accuracy: 0.8189 - val_loss: 0.9347\n","782/782 - 42s - 54ms/step - accuracy: 0.8189 - loss: 0.9347\n","Pérdida de prueba: 0.9347452521324158\n","Exactitud de prueba: 0.8189200162887573\n"]}],"source":["# Entrenamiento del modelo\n","model.fit([train_texts_padded, features_train_scaled], train_labels, \n","          batch_size=32, \n","          epochs=15, \n","          validation_data=([test_texts_padded, features_test_scaled], test_labels), \n","          verbose=2)\n","\n","# Evaluación del modelo\n","test_loss, test_accuracy = model.evaluate([test_texts_padded, features_test_scaled], test_labels, batch_size=32, verbose=2)\n","print('Pérdida de prueba:', test_loss)\n","print('Exactitud de prueba:', test_accuracy)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
